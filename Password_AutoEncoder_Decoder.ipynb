{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Password-AutoEncoder-Decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMU9M+Q0B4yx6h0+ILCAiFX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsTurner/ben-learns-ml/blob/main/Password_AutoEncoder_Decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDP8bnvJnht7"
      },
      "source": [
        "Necessary import statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LT8C3qOnkCr"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA as skPCA\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds_je3a8MFyL"
      },
      "source": [
        "Import frequent words data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5meeviWpMqC0",
        "outputId": "2dec5089-beb0-47ca-c84f-051bea34f287"
      },
      "source": [
        "fullWordsList = pd.read_csv('unigram_freq.csv')\n",
        "wordsList = fullWordsList[['word']]\n",
        "wordsArray = np.array(wordsList['word'])\n",
        "\n",
        "\n",
        "# for index, element in enumerate(wordsArray):\n",
        "#   try:\n",
        "#     if len(element) == 6:\n",
        "#       #print(\"converter\",converter(element))\n",
        "#       fiveLengthValues[index] = converter(element)\n",
        "#       #print(\"five length\",fiveLengthValues[index])\n",
        "#       #fiveLengthArray = np.append(fiveLengthArray, element)\n",
        "#       # print(converter(list(element)))\n",
        "#   except:\n",
        "#     print(element, \"fucked it up\")\n",
        "# #print(fiveLengthValues)\n",
        "\n",
        "\n",
        "#print()\n",
        "fiveLengthArray = formatter(wordsArray, 6)\n",
        "fiveLengthValues = np.zeros((len(fiveLengthArray), 6))\n",
        "\n",
        "fiveLengthValues = converter(fiveLengthArray, 6)\n",
        "print(fiveLengthValues)\n",
        "#print(fiveLengthArray)\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nan fucked it up with error of object of type 'float' has no len()\n",
            "nan fucked it up with error of object of type 'float' has no len()\n",
            "[[0.69230769 0.15384615 0.         0.65384615 0.07692308 0.26923077]\n",
            " [0.53846154 0.5        0.42307692 0.30769231 0.5        0.15384615]\n",
            " [0.57692308 0.15384615 0.53846154 0.57692308 0.42307692 0.15384615]\n",
            " ...\n",
            " [0.23076923 0.53846154 0.53846154 0.19230769 0.15384615 0.42307692]\n",
            " [0.23076923 0.53846154 0.53846154 0.11538462 0.11538462 0.23076923]\n",
            " [0.23076923 0.53846154 0.42307692 0.42307692 0.23076923 0.53846154]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLnFdVGBlCg2",
        "outputId": "741ec8a9-f68b-4266-b2da-665ceae357cf"
      },
      "source": [
        "print(fiveLengthValues[100])"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPdBnbwYm9Di"
      },
      "source": [
        "Take input and format to be used generalized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1eE6ZCem2E4",
        "outputId": "a55a487b-aede-4d30-bc37-ccb370f11f1f"
      },
      "source": [
        "print(\"Input password of length 5\")\n",
        "password = \"hello\"\n",
        "password = password.lower()\n",
        "\n",
        "passwordValue = np.empty(len(password))\n",
        "\n",
        "# passwordArray = np.array([])\n",
        "# for index, element in enumerate(reversed(password)):\n",
        "#   passwordArray = np.append(element, passwordArray)\n",
        "\n",
        "alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "\n",
        "for index, element in enumerate(password):\n",
        "  passwordValue[index] = alphabet.index(element)/len(alphabet)\n",
        "\n",
        "print(passwordValue)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input password of length 5\n",
            "[0.26923077 0.15384615 0.42307692 0.42307692 0.53846154]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_xWDEkxu4bM"
      },
      "source": [
        "Function to take a string and convert to the values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSd8S0lFu8Rg"
      },
      "source": [
        "def calculator(input):\n",
        "  values = np.empty(len(input))\n",
        "  for index, element in enumerate(input):\n",
        "    values[index] = alphabet.index(element)/len(alphabet)\n",
        "  return values\n",
        "\n",
        "\n",
        "def formatter(input, length):\n",
        "  formattedArray = np.array([])\n",
        "  for index, element in enumerate(input):\n",
        "    try:\n",
        "      if len(element) == length:\n",
        "        element = [element]\n",
        "        x = np.array(element)\n",
        "\n",
        "        formattedArray = np.append(formattedArray, x, axis=0)\n",
        "    except Exception as e:\n",
        "      print(element, \"fucked it up\", \"with error of\", e)\n",
        "      #print(\"element is shape\", np.array(list(element)).shape)\n",
        "  return formattedArray\n",
        "\n",
        "def converter(input, length):\n",
        "  valuesArray = np.zeros((len(input), length))\n",
        "  for index, element in enumerate(input):\n",
        "    try:\n",
        "      valuesArray[index] = calculator(element)\n",
        "    except Exception as e:\n",
        "      print(element, \"fucked it up\", \"with error of\", e)\n",
        "  return valuesArray\n"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOHxnt9OvYeU"
      },
      "source": [
        "Generate array of strings of length 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRssluwevgC_",
        "outputId": "4a53f27d-a64c-4724-b0d1-4597a0345949"
      },
      "source": [
        "def generator(length):\n",
        "  string = []\n",
        "  for i in range(length):\n",
        "    string.append(random.choice(alphabet))\n",
        "  return string\n",
        "print(generator(5))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['p', 'k', 'w', 'h', 'g']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chNSQnthwiVH"
      },
      "source": [
        "Next initialize your own data set of random strings that have been converted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcipVXlDwmde"
      },
      "source": [
        "# x_train = np.array(np.zeros(([1000, 6])))\n",
        "# #y_train = np.array(np.zeros(([1000, 1])))\n",
        "# for i in range(len(x_train)):\n",
        "#   values = converter(fiveLengthArray[i])\n",
        "#   values = np.array([values])\n",
        "#   #expectedPrediction = encoder.predict(values)\n",
        "#   x_train[i] = values\n",
        "#   #y_train[i] = expectedPrediction\n",
        "\n",
        "# print(x_train)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bLrhsMJBfLS"
      },
      "source": [
        "##Tavin Autoencoder Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-TO43k8BnNT"
      },
      "source": [
        "encoder_input_dim = (6,)\n",
        "latent_space_dim = 1\n",
        "\n",
        "encoder_input = layers.Input(shape=encoder_input_dim, name='encoder_input')\n",
        "\n",
        "x = encoder_input\n",
        "\n",
        "encoder_layer_size = [4, 2]\n",
        "\n",
        "for i in range(len(encoder_layer_size)):\n",
        "  dense_layer = layers.Dense(encoder_layer_size[i], name=\"encoder_conv_\" + str(i))\n",
        "  x = dense_layer(x)\n",
        "  x = layers.LeakyReLU()(x)\n",
        "  #x = activations.selu(x)\n",
        "\n",
        "encoder_output = layers.Dense(latent_space_dim)(x)\n",
        "\n",
        "encoder = keras.Model(encoder_input, encoder_output)\n",
        "\n",
        "#encoder.summary()"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeVsMyxmBxC_"
      },
      "source": [
        "decoder_input = layers.Input(shape=(latent_space_dim,), name='decoder_input')\n",
        "\n",
        "x = decoder_input\n",
        "\n",
        "decoder_layer_size = [2, 4, 6]\n",
        "\n",
        "for i in range(len(decoder_layer_size)):\n",
        "  dense_layer = layers.Dense(decoder_layer_size[i], name=\"encoder_conv_\" + str(i))\n",
        "  x = dense_layer(x)\n",
        "  x = activations.sigmoid(x)\n",
        "\n",
        "decoder_output = x\n",
        "\n",
        "decoder = keras.Model(decoder_input, decoder_output)\n",
        "\n",
        "#decoder.summary()"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU1SkVGSB05K"
      },
      "source": [
        "model_input = encoder_input\n",
        "model_output = decoder(encoder_output)\n",
        "\n",
        "model = keras.Model(model_input, model_output)\n",
        "\n",
        "#model.summary()"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mEY9KzeB1hZ"
      },
      "source": [
        "learning_rate= 0.4\n",
        "\n",
        "optimizer = optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "def r_loss(y_true, y_pred):\n",
        "  return keras.mean(keras.square(y_true - y_pred))\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=losses.MeanSquaredError())"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnetww7lB3h6",
        "outputId": "cf32f7b0-bb31-460f-aad6-10fe010f2c6c"
      },
      "source": [
        "model.fit(\n",
        "    x = fiveLengthValues,\n",
        "    y = fiveLengthValues,\n",
        "    # x = x_train,\n",
        "    # y = x_train,\n",
        "    batch_size = 10000,\n",
        "    shuffle = True,\n",
        "    epochs = 100,\n",
        "    verbose = 0\n",
        ")"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f85d4edca58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zECYd0A6B9bO",
        "outputId": "373cbaa7-9f6f-4a2c-c896-156a7a1c359f"
      },
      "source": [
        "sampleString = [\"r\", \"a\", \"n\", \"d\", \"o\", \"m\"]\n",
        "#, \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\"]\n",
        "sampleValues = calculator(sampleString)\n",
        "sampleValues = np.array([sampleValues])\n",
        "\n",
        "midway = encoder.predict([sampleValues])\n",
        "print(midway)\n",
        "output = decoder.predict([midway])\n",
        "print(output)\n",
        "\n",
        "output = output*len(alphabet)\n",
        "print(output)\n"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f85dbd46268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[[-8.968789]]\n",
            "WARNING:tensorflow:7 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f85dbd34ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[[0.4069006  0.35988975 0.4427969  0.41301376 0.35681272 0.45948163]]\n",
            "[[10.579416  9.357134 11.512719 10.738358  9.27713  11.946523]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}